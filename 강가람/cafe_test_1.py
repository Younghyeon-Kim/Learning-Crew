# -*- coding: utf-8 -*-
"""cafe_test_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WWrZXfjbivmT7XrppbLI7onSFgOeTw2F
"""

!pip install sentencepiece

from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import pandas as pd
import sentencepiece as spm
import re
import os
import pickle

path = "/content/drive/MyDrive/BIGDATA_STUDY/project_NLP/"
#파일 불러오기(경로명은 각자)
df1 = pd.read_csv(path + "카페_validation.csv")
df2 = pd.read_csv(path + "카페_train.csv")
os.getcwd()

#c-a , s-q 지우기
no_data1 = df1[ (df1['발화자'] == 's') & (df1['QA여부'] == 'q')].index
no_data2 = df1[ (df1['발화자'] == 'c') & (df1['QA여부'] == 'a')].index
df1_dr = df1.drop(no_data1)


no_data3 = df2[ (df2['발화자'] == 's') & (df2['QA여부'] == 'q')].index
no_data4 = df2[ (df2['발화자'] == 'c') & (df2['QA여부'] == 'a')].index
df2_dr = df2.drop(no_data3)

df1_dr.drop(no_data2, inplace = True)
df1_dr.reset_index(inplace = True, drop = True)

df2_dr.drop(no_data4, inplace = True)
df2_dr.reset_index(inplace = True, drop = True)

#인텐트 항목 앞글자만 자르기
lst1 = list(df1_dr['인텐트'])
lst2 = [i.split('_')[0] for i in lst1]
df1_dr['인텐트'] = lst2

lst3 = list(df2_dr['인텐트'])
lst4 = [i.split('_')[0] for i in lst3]
df2_dr['인텐트'] = lst4

# qa 정렬대로 뽑아오기 
Q = []
A = []
Intent = []

for i in df1_dr.index:
  if df1_dr['QA여부'][i] == 'q' and df1_dr['QA여부'][i+1] == 'a':
    Q.append(df1_dr.loc[i]['발화문'])
    A.append(df1_dr.loc[i+1]['발화문'])
    Intent.append(df1_dr.loc[i]['인텐트'])

cafe_train_data= pd.DataFrame({'Q':Q,
                'A':A,
                'Intent':Intent})

# qa 정렬대로 뽑아오기
Q = []
A = []
Intent = []

for i in df2_dr.index:
  if df2_dr['QA여부'][i] == 'q' and df2_dr['QA여부'][i+1] == 'a':
    Q.append(df2_dr.loc[i]['발화문'])
    A.append(df2_dr.loc[i+1]['발화문'])
    Intent.append(df2_dr.loc[i]['인텐트'])

cafe_train_data= pd.DataFrame({'Q':Q,
                'A':A,
                'Intent':Intent})

cafe_train_data

"""#2. 데이터 분리"""

question, answer = list(cafe_train_data['Q']), list(cafe_train_data['A'])

# 특수 문자를 제거한다.
FILTERS = "([~.,!?\"':;)(])"
question = [re.sub(FILTERS, "", s) for s in question]
answer = [re.sub(FILTERS, "", s) for s in answer]

cafe_train_data.head()

# # 학습 데이터와 시험 데이터를 분리한다.
# que_train, que_test, ans_train, ans_test = train_test_split(question, answer, test_size=0.1, random_state=0)

# que_train[0], ans_train[0]

question[0], answer[0]

# Sentencepice용 사전을 만들기 위해 que_train + que_test를 저장해 둔다.
data_file = "/content/drive/MyDrive/BIGDATA_STUDY/NLP/chatbot/chatbot_data.txt"
with open(data_file, 'w', encoding='utf-8') as f:
    for sent in question + answer:
        f.write(sent + '\n')

# Google의 Sentencepiece를 이용해서 vocabulary를 생성한다.
# -----------------------------------------------------
templates= "--input={} \
            --pad_id=0 --pad_piece=<PAD>\
            --unk_id=1 --unk_piece=<UNK>\
            --bos_id=2 --bos_piece=<BOS>\
            --eos_id=3 --eos_piece=<EOS>\
            --model_prefix={} \
            --vocab_size={}"

VOCAB_SIZE = 9000
model_prefix = "/content/drive/MyDrive/BIGDATA_STUDY/NLP/chatbot/chatbot_model"
params = templates.format(data_file, model_prefix, VOCAB_SIZE)

spm.SentencePieceTrainer.Train(params)
sp = spm.SentencePieceProcessor()
sp.Load(model_prefix + '.model')

with open(model_prefix + '.vocab', encoding='utf-8') as f:
    vocab = [doc.strip().split('\t') for doc in f]

word2idx = {k:v for v, [k, _] in enumerate(vocab)}
idx2word = {v:k for v, [k, _] in enumerate(vocab)}

word2idx

# 학습 데이터를 생성한다. (인코더 입력용, 디코더 입력용, 디코더 출력용)
MAX_LEN = 15
enc_input = []
dec_input = []
dec_output = []

for Q, A in zip(question, answer):
    # Encoder 입력
    enc_i = sp.encode_as_ids(Q)
    enc_input.append(enc_i)

    # Decoder 입력, 출력
    dec_i = [sp.bos_id()]   # <BOS>에서 시작함
    dec_o = []
    for ans in sp.encode_as_ids(A):
        dec_i.append(ans)
        dec_o.append(ans)
    dec_o.append(sp.eos_id())   # Decoder 출력은 <EOS>로 끝남.        
    
    # dec_o는 <EOS>가 마지막에 들어있다. 나중에 pad_sequences()에서 <EOS>가
    # 잘려 나가지 않도록 MAX_LEN 위치에 <EOS>를 넣어준다.
    if len(dec_o) > MAX_LEN:
        dec_o[MAX_LEN] = sp.eos_id()
        
    dec_input.append(dec_i)
    dec_output.append(dec_o)

enc_input[1]

dec_input[1]

dec_output[1]

# 각 문장의 길이를 맞추고 남는 부분에 padding을 삽입한다.
enc_input = pad_sequences(enc_input, maxlen=MAX_LEN, value = sp.pad_id(), padding='post', truncating='post')
dec_input = pad_sequences(dec_input, maxlen=MAX_LEN, value = sp.pad_id(), padding='post', truncating='post')
dec_output = pad_sequences(dec_output, maxlen=MAX_LEN, value = sp.pad_id(), padding='post', truncating='post')

# 사전과 학습 데이터를 저장한다.
with open('/content/drive/MyDrive/BIGDATA_STUDY/NLP/chatbot/chatbot_cafe_test.pkl', 'wb') as f:
    pickle.dump([word2idx, idx2word], f, pickle.HIGHEST_PROTOCOL)

# BLEU 평가를 위해 que_test와 ans_test를 저장해 둔다.
with open('/content/drive/MyDrive/BIGDATA_STUDY/NLP/chatbot/chatbot_cafe_test_train.pkl', 'wb') as f:
    pickle.dump([enc_input, dec_input, dec_output], f, pickle.HIGHEST_PROTOCOL)

from google.colab import drive
drive.mount('/content/drive')